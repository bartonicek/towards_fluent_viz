---
title: "Towards Fluent Interactive Data Visualization"
author: "Adam Bartonicek"
date: "`r Sys.Date()`"
output: bookdown::pdf_document2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## What do we interact with?

There is a subtle yet profound question in the production and use of interactive data visualizations: *when we interact with a plot, what exactly are we interacting with?* On its face, the question might seem trivial. A person clicking a bar in an interactive barplot may be convinced that they are interacting with the coloured rectangle on the screen, since, by design, that is the thing they see change on the screen in front of them. And in some way, this is true - by interacting with the bar, we can affect its graphical attributes: we can change its colour, we can squeeze it/stretch it, and so on. Yet, in another, deeper way, this perception of interacting with a plain geometric object is just an illusion. How so? The illusion lies in the fact that the coloured rectangle representing the bar is not meaningful on its own. Instead, the bar is only ever meaningful as a bar within the context of the plot. We can see this easily. If we were to take the coloured rectangle and transpose it onto a blank area of the screen, we would lose some crucial information.  

The statement that geometric objects are only meaningful within the context of the plot should not come as surprising or controversial to people familiar with data visualization. Despite that, it may be somewhat challenging to define in detail what exactly this context that the plot provides is. We know that the geometric objects in a plot are supposed to represent some underlying data. That much is clear - if objects in a graphic do not represent any external information but are instead drawn according to some arbitrary rules, we cannot really, in good conscience, call it a "plot". But data is only a part of the story. When drawing plots, we rarely represent the raw data directly. Instead, we often summarize, aggregate, or transform the data, by applying mathematical functions such as count, sum, mean, log, or the quantile function. 

The output of these transformations is what we then represent by the geometric objects. So, when interacting with a bar in an interactive barplot, we are not just interacting with a plain geometric object, we are interacting with a mathematical function, or, in fact, several of them. This is important since functions have properties. The core argument of the present text is that the properties of these functions impose limits on what kinds of visualizations and interactions can be meaningfully composed. Before diving deeper, however, let's first define some key terms and draw a rough sketch of the data visualization process. 

## Rough sketch of the data visulization process

To create a data visualization, be it static or interactive, we need several key ingredients: data, summaries, scales/coordinate systems, and geometric objects. First of all, as was mentioned above, every data visualization needs to be built on top of some underlying data. We can represent this as a set of some arbitrary units of information $D$. Data in the wild usually comes with more structure than that - for example, we often encounter data stored in a tabular ( or "tidy" [CITE]) format, stratified by rows and columns. In that case, we could substitute $D$ by the set of rows $R$, the set of columns $C$, or the set of cell values $R \times C$ (where $\times$ indicates the cartesian product). However, for the purpose of this description, we do not have to assume any special structure and just speak of the data units $d \in D$.  

Secondly, the set of data units $D$ is transformed into a set of collections of summaries $S$ via some function $\alpha$. It may be the case that $\alpha$ is one-to-one (bijection/isomorphism), in which case there is one unit of data for every summary. Examples of this include the typical scatterplot, in which $\alpha$ is the identity function. However, more often, $\alpha$ is many-to-one (surjection), which means that each summary may be composed of multiple units of data. This is the case for the typical barplot, histogram, density plot, or violin plot (also kernel density). When the function is many-to-one, it will typically reduce the cardinality of the set at hand such that $\lvert S \lvert \leq \lvert D \lvert$ (i.e. in a typical barplot, there will be fewer bars than there are rows of the data, unless each row has a unique value of the categorical variable). This is done by stratifying on one or more variables which may either come from the data directly (as in the case of a barplot or a treemap) or may themselves be a summary of the data (as in the case of histogram bins). Importantly also, each collection of summaries $s \in S$ may (and usually will) hold multiple values, produced by a different constituent function each - for example, the collection $s$ for a single boxplot "box" will consist of a median, first and third quartile, and the minimum and maximum of some variable, for a given level of some stratifying variable (which itself will also be an element of $s$). The output of these constituent functions may also depend on some external parameters, such as anchor and binwidth in a histogram.

Thirdly, each collection of summaries $s \in S$ needs to be translated from the data- (or summary-) coordinates to graphical coordinates/attributes $g \in G$, via a function $\beta$. This means that each summary value gets mapped or "scaled" to a graphical attribute via a constituent scaling function. Note that this mapping preserves cardinality - there are as many collections of graphical attributes as there are collections of summaries, $\lvert G \lvert = \lvert S \lvert$. For numeric/continuous summaries, scales typically come in the form of linear transformations, such that the minimum and maximum of the data are mapped near the minimum and maximum of the plotting region, respectively. Continuous scales may also provide non-linear trasnformations such as log-transformation or binning. For discrete summaries, scales can also map values onto continuous graphical attributes, or onto discrete ones.   

Finally, the collections of graphical coordinates $g \in G$ are drawn as geometric objects inside the plotting region, which we can represent as the set of pixels $P$, via the graphic device. While the act of drawing does take the collections of graphical coordinates $g \in G$ as inputs, it does not simply return an output for each input (like a mathematical function would), but instead mutates the state of the graphical device via a side effect $\gamma^*$, i.e. changing the colour values of pixels. In other words, how the graphic ends up looking may depend on the order in which we draw things, for example, some geometric objects may end up being plotted over others, and so $\gamma^*$ is not a simple mapping from $G$ to $P$. As such, we cannot call $\gamma^*$ a true mathematical function. The geometric objects may be simple, such as points, lines, or bars, or compound, such as a boxplot or pointrange. Importantly, each attribute necessary to draw the geometric object, such as x- and y-position, width, height, area, etc... needs to be present in the corresponding $g$.     

The whole process can be summarized as follows:

$$D \overset{\alpha}{\to} S \overset{\beta}{\to} G \overset{\gamma^*}{\Rightarrow} P$$

Or, equivalently: 

$$\text{(data)} \overset{\text{summarize}}{\longrightarrow} \text{(summaries)} \overset{\text{translate/encode}}{\longrightarrow} \text{(graph. coordinates)} \overset{\text{draw}^*}{\Rightarrow} \text{(graph. device state)}$$

The above should be fairly non-controversial description of how a data visualization is produced, and applies equally well to static as well as interactive visualizations. 

## Why summaries matter

In some treatments of data visualization, the summarizing step ($\alpha: D \to S$) is de-emphasized, in one of two ways. In the first case, the data is already presumed to arrive pre-summarized (i.e. $S = D$, for example we can draw a barplot from a pre-summarized table of counts), and data visualization is just about encoding the data into graphical coordinates. This framing feels especially natural when considering plots which show a 1-to-1 mapping (bijection) between the data and the geometric objects, such as the scatterplot or the lineplot/time-series chart. Indeed, in these plots, the summarizing function is identity, and as such *can* be ignored. In the second case, the computation of summaries is considered, however, it is absorbed into the translation step, such that $\alpha = \beta$ (e.g., a histogram may be thought of as directly translating its underlying variable into bin coordinates). Both of these approaches produce a tight coupling between the summaries and graphical coordinates.

However, we can give a much richer account of visualizations by treating both the summarizing and the translating step as first class citizens. Crucially, some plots may encode the same summaries through different graphical attributes and geometric objects; others may compute different summaries but display them via the same means. As an example of the former, see the example of a histogram and spineplot in Figure \@ref(fig:histospine). Histograms and spineplots use the same summaries, i.e. binned counts of some underlying continuous variable $x$. However, histograms encodes the bin breaks into rectangle boundaries along the x-axis, and the counts as the top rectangle boundary along the y-axis, whereas spineplot encodes the counts into both x- and y-axis rectangle boundaries, with both the x- and the y- dimensions stacked and the y-dimension additionally scaled to 1, and the bin breaks are encoded as x-axis labels. As an example of the latter, scatterplot and bubbleplot both use points/circles to display their underlying summaries, however, these differ substantially.  

```{r histospine, fig.height=3, echo=FALSE, fig.cap="Histogram and spineplot use the same summaries but encode them in different ways. a) In histogram, the x-axis display bin breaks and y-axis displays count, stacked across groups (failure/no failure). b) In spineplot, the x-axis and y-axis both display count; the x-axis shows count stacked across bins, whereas the y-axis shows count stacked across groups and scaled by the total bin count (such that the total bin height = 1). The bin breaks are displayed as x-axis labels (however, the underlying summary is still stacked count)."}

fail <- factor(c(2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1,
                 1, 1, 1, 2, 1, 1, 1, 1, 1),
               levels = c(1, 2), labels = c("no", "yes"))
temperature <- c(53, 57, 58, 63, 66, 67, 67, 67, 68, 69, 70, 70,
                 70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 81)

breaks <- seq(50, 85, 5)
h1 <- hist(temperature[fail == "yes"], breaks = breaks, plot = FALSE)
h2 <- hist(temperature[fail == "no"], breaks = breaks, plot = FALSE)
h2$counts <- h2$counts + h1$counts

axis_fun1 <- function() {
  axis(1, tick = FALSE, line = -0.5)
  axis(2, tick = FALSE, las = 1, line = -0.5)
  box(bty = "L")
}

cols <- c("grey80", "indianred")
border <- "white"

par(mfrow = c(1, 2))
plot(h2, main = NULL, xlab = "Temperature", ylab = "Count", 
     axes = FALSE, col = cols[1], border = border)
plot(h1, add = TRUE, col = cols[2], border = border)
legend("topleft", legend = c("Failure", "No failure"), col = cols,
       pch = 15, bty = "n", cex = 0.75)
title(main = "a.", adj = 0, line = 0.5)
axis_fun1()

png()
s1 <- invisible(spineplot(fail ~ temperature))
invisible(dev.off())
cumtotal <- c(0, cumsum(rowSums(s1)) / sum(s1))

spineplot(fail ~ temperature, breaks = breaks, xlab = "Temperature", 
          ylab = "Proportion", col = rev(cols), border = border, axes = FALSE)
axis(1, at = cumtotal, labels = breaks, tick = FALSE, line = -0.5)
axis(2, at = 0:4 / 4, labels = 0:4 / 4, tick = FALSE, las = 1, line = -0.5)
title(main = "b.", adj = 0, line = 0.5)
box(col = "white", lwd = 1.5)
box(bty = "L")

```

## A couple of caveats

There are also a few important caveats to the rough sketch of the data visualization process outlined above. Firstly, while not always the case, *order* can be important. Within each $s \in S$ and $g \in G$, some summaries may be ordered lists of values, as in the case of the lineplot (i.e. lines need to be drawn by connecting a series of points in the correct order). Secondly, the collections in $S$ (and in turn, in $G$) may themselves be ordered and form a hierarchical or tree-like structure. This is particularly relevant in the case of stacking. For example, when drawing a stacked barplot, we need to stack the graphical coordinate values representing the sub-bars on top of each other, such that each sub-bar is stacked within the appropriate parent-bar and in the correct order (e.g. such that sub-bars belonging to group 2 always go on top group 1 sub-bars). If it is the case that order matters for either $\alpha$ or $\beta$, then we cannot call them true functions either, since each of the elements in their domain needs to be "aware" of the other elements, or of what has happened before.   

Finally, the data limits (minimum and maximum) and values that are used for scales are often derived from $S$ rather than from the raw data (e.g. the upper y-axis limit in barplot or histogram is the highest count across bins). Further, the limits may come from a higher level of hierarchy than the summaries that are actually being drawn - for example, in a stacked barplot, for the upper y-axis limit we need to know the count (height) of the tallest *whole* bar but do not need to know the counts within the stacked sub-bars (since these are, by definition, smaller or equal to the whole bar). 

This is where the fundamental differences between static and interactive visualizations become very relevant. In static visualizations, all computation is done only once, before the plot is rendered, and so the issues of order, hierarchical structure of the summaries, and the tracking of axis limits are less important. Interactive visualizations, on the other hand, need to reactively respond to the user's input, and some computations may need to run many times within a single second. As a result, it is imperative to organize the process in such a way that we do only as little work as is necessary.        

# Interactivity and Hierarchy

As was briefly mentioned in the previous section, interactive graphics come with their own set of considerations around hierarchy and efficiency. Specifically, computations which result from interaction may need to occur repeatedly within short time frames. 


# The Problem of Statistical Summaries

There is an even deeper issue when it comes to interactivity and statistical summaries of the data. Specifically, not every statistical summary "works" equally well - instead, some summaries may be better than other. Let's first illustrate the problem with an example.

Linked brushing or highlighting is one of the most popular types of interactive features used in interactive data visualizations [CITE]. It allows the user to select objects (such as points or bars) within one plot by e.g. clicking or clicking-and-dragging, and the corresponding cases (rows of the data) are then highlighted in all other plots. Its usefulness comes from the fact that allows the user to rapidly "drill-down" [CITE] and explore the summaries that would result from subsetting different rows of the data, within the context of the entire dataset.  

Now, let's imagine we have three interactive plots: a classical scatterplot, a barplot of summarizing the count of cases within the levels of some categorical variable $x$, and a barplot summarizing the mean of some other variable $y$, within levels of the same categorical variable $x$. The plots are linked such that they allow for linked brushing/highlighting. Intuitively, it might seem that the barplot of counts and the barplot of means are equally valid/useful representations of the underlying data. However, if we consider these plots in the context of linked brushing, few subtle-yet-fundamental differences become apparent.  

```{r empty, echo=FALSE, fig.height=3, fig.cap="The problem of representing empty selection. a) An illustration of selection by linked brushing. b) In the barplot of counts, the count within an empty selection (red) is zero and so an absence of a bar accurately represents a count of zero. c) In the barplot of means, the mean of an empty selection is not defined. Absence of a bar could indicate that either no cases are selected or some cases are selected and their mean is equal to the lower y-axis limit."}

wt <- mtcars$wt
mpg <- mtcars$mpg
cyl <- mtcars$cyl
disp <- mtcars$disp
one <- numeric(nrow(mtcars)) + 1

cyl_unique <- c(4, 6, 8)

wt_lim0 <- 1.45
wt_lim1 <- 2.5
mpg_lim0 <- 25
mpg_lim1 <- 34.5

cyl_unique <- c(4, 6, 8)

selected <- ((wt > wt_lim0) & (wt < wt_lim1) & (mpg > mpg_lim0) & mpg < mpg_lim1)
col <- c("grey80", "indianred")[selected + 1]
selected_cyls <- c(4, 6, 8) %in% unique(cyl[selected])

bars11 <- tapply(one, cyl, sum)
bars12 <- tapply(one[selected], cyl[selected], sum)

bars21 <- tapply(disp, cyl, mean)
bars22 <- tapply(disp[selected], cyl[selected], mean)

axis_fun1 <- function() {
  axis(1, tick = FALSE, line = -0.5)
  axis(2, tick = FALSE, las = 1, line = -0.5)
  box(bty = "L")
}

axis_fun2 <- function() {
  axis(1, at = c(4, 6, 8), tick = FALSE, line = -0.5)
  axis(2, tick = FALSE, las = 1, line = -0.5)
  box(bty = "L")
}

par(mfrow = c(1, 3), mar = c(5, 4, 2, 0.5))

plot(wt, mpg, pch = 20, col = col, cex = 2,
     axes = FALSE, xlab = "Weight", ylab = "Mileage")
rect(wt_lim0, mpg_lim0, wt_lim1, mpg_lim1, lty = "dashed")
title(main = "a.", adj = 0, line = 0.5)
axis_fun1()

barx <- c(4, 6, 8)
barx0 <- barx - 0.75
barx1 <- barx + 0.75

barx_selected<- barx[selected_cyls]
barx_notselected <- barx[!selected_cyls]
barx0_selected <- barx0[selected_cyls]
barx1_selected <- barx1[selected_cyls]
barx0_notselected <- barx0[!selected_cyls]
barx1_notselected <- barx1[!selected_cyls]

plot(c(2, 10), c(0, max(bars11)), type = "n", 
     axes = FALSE, xlab = "Cylinders", ylab = "Count")
rect(barx0, 0, barx1, bars11,
     col = "grey80", border = NA)
rect(barx0_selected, 0, barx1_selected, bars12,
     col = "indianred", border = NA)
text(barx, c(bars12, 0, 0) + 0.75, labels = c(bars12, "0", "0"),
     col = "indianred")
title(main = "b.", adj = 0, line = 0.5)
axis_fun2()

plot(c(2, 10), c(0, max(bars21)), type = "n",
     axes = FALSE, xlab = "Cylinders", ylab = "Mean displacement")
rect(cyl_unique - 0.75, 0, cyl_unique + 0.75, bars21,
     col = "grey80", border = NA)
rect(barx0_selected, 0, barx1_selected, bars22,
     col = "indianred", border = NA)
rect(barx0_notselected, 0, barx1_notselected, 100,
     col = NA, border = "indianred", lty = "dashed")
text(barx_notselected, 50, labels = "?", col = "indianred", cex = 1.5)
text(barx_selected, bars22 + 10, labels = round(bars22), col = "indianred")
title(main = "c.", adj = 0, line = 0.5)
axis_fun2()

```

## Empty selections

Firstly, as is shown in Figure \@ref(fig:empty), how do we draw an empty selection? In the case of counts, we have a meaningful default value - zero - as in "the number of cases in an empty set is 0". When we seen an absence of a bar in a barplot, we know that the count for the corresponding level of the stratifying variables.

However, there is no similar default value for means: the mean of an empty set is not defined. We could just not draw the bar representing the empty selection, however, that decouples the statistical summary from the visual representation: the absence of a bar may now indicate that *either* no cases are selected *or* that some cases are selected and their mean is equal to the lower y-axis limit. 

```{r bigger, echo=FALSE, fig.height=3, fig.cap="The relationship between selection vs. the whole. a) An illustration of selection by linked brushing. b) In the barplot of counts (middle), the count within a selection (red) is always less than or equal to the count within the whole and the outline of the bars does not change. c) In the barplot of means, the mean of a selection can be greater and so the outline of the bars will change in response to user input."}

wt <- mtcars$wt
mpg <- mtcars$mpg
cyl <- mtcars$cyl
disp <- mtcars$disp
one <- numeric(nrow(mtcars)) + 1

cyl_unique <- c(4, 6, 8)

wt_lim0 <- 2
wt_lim1 <- 3.5
mpg_lim0 <- 20
mpg_lim1 <- 26.5

selected <- ((wt > wt_lim0) & (wt < wt_lim1) & (mpg > mpg_lim0) & mpg < mpg_lim1)
col <- c("grey80", "indianred")[selected + 1]
selected_cyls <- c(4, 6, 8) %in% unique(cyl[selected])

bars11 <- tapply(one, cyl, sum)
bars12 <- tapply(one[selected], cyl[selected], sum)

bars21 <- tapply(disp, cyl, mean)
bars22 <- tapply(disp[selected], cyl[selected], mean)

axis_fun1 <- function() {
  axis(1, tick = FALSE, line = -0.5)
  axis(2, tick = FALSE, las = 1, line = -0.5)
  box(bty = "L")
}

axis_fun2 <- function() {
  axis(1, at = c(4, 6, 8), tick = FALSE, line = -0.5)
  axis(2, tick = FALSE, las = 1, line = -0.5)
  box(bty = "L")
}

par(mfrow = c(1, 3), mar = c(5, 4, 2, 0.5))

plot(wt, mpg, pch = 20, col = col, cex = 2,
     axes = FALSE, xlab = "Weight", ylab = "Mileage")
rect(wt_lim0, mpg_lim0, wt_lim1, mpg_lim1, lty = "dashed")
title(main = "a.", adj = 0, line = 0.5)
axis_fun1()

barx <- c(4, 6, 8)
barx0 <- barx - 0.75
barx1 <- barx + 0.75

barx_selected<- barx[selected_cyls]
barx_notselected <- barx[!selected_cyls]
barx0_selected <- barx0[selected_cyls]
barx1_selected <- barx1[selected_cyls]
barx0_notselected <- barx0[!selected_cyls]
barx1_notselected <- barx1[!selected_cyls]

plot(c(2, 10), c(0, max(bars11)), type = "n", 
     axes = FALSE, xlab = "Cylinders", ylab = "Count")
rect(barx0, 0, barx1, bars11,
     col = "grey80", border = NA)
rect(barx0_selected, 0, barx1_selected, bars12,
     col = "indianred", border = NA)
text(barx, c(bars12, 0, 0) + 0.75, labels = c(bars12, "0", "0"),
     col = "indianred")
title(main = "b.", adj = 0, line = 0.5)
axis_fun2()

plot(c(2, 10), c(0, max(bars21)), type = "n",
     axes = FALSE, xlab = "Cylinders", ylab = "Mean displacement")
rect(cyl_unique - 0.75, 0, cyl_unique + 0.75, bars21,
     col = "grey80", border = NA)
rect(barx0_selected, 0, barx1_selected, bars22,
     col = adjustcolor("indianred", 0.5), border = NA)
rect(barx0_notselected, 0, barx1_notselected, 100,
     col = NA, border = "indianred", lty = "dashed")
text(barx_notselected, 50, labels = "?", col = "indianred", cex = 1.5)
text(barx_selected, bars22 + 10, labels = paste0(round(bars22), "!"), col = "indianred")
title(main = "c.", adj = 0, line = 0.5)
axis_fun2()

```

## Part vs. the whole

Second, as shown in Figure \@ref(fig:bigger), how does the summary on the selection relate to the summary on the whole? In the case of the barplot of counts, the height of the sub-bar is always less than or equal ($\leq$) to the height of the whole bar (because so is the count). Thus, we can always draw the sub-bar over the whole bar, and the whole bar act as a stable visual reference - the outline of the whole bar will remain the same no matter which cases of the data are selected. In fact, we can either draw the whole bar (as shown in grey in Figure \@ref(fig:bigger)) and draw the selected sub-bar (red) over it, both starting from the y-axis origin (0), or we can draw a red sub-bar and the "leftover" grey-sub bar stacked on top of it, starting from the top y-coordinate of the selection bar and with $\textbf{height} = (\text{count of the whole} - \text{count of the selection})$. The resulting plots will be identical, visually. 

The barplot of means, does not share these nice properties. Specifically, the "sub-bars" can be taller than the whole bar (because the mean of a subset can be greater than the mean of the original set). As a result, if we draw the selection sub-bars on top of the whole bar, the whole bar may become completely obscured by it (as shown in Figure \@ref(fig:bigger)). We could choose to draw the selection sub-bars as semitransparent, or draw the the bars side-by-side instead of on top of each other (dodging), however, the question then remains how to display the non-selected (grey) cases - do we draw the "whole" bar that remains the same height throughout interaction, or the "leftover" bar whose height changes with the selection? Also, if we choose to draw the bars side-by-side, will the whole bar be initially wide and shrink in response to selection to accommodate the selection bars, or will it be narrow from the initial render? Finally, if the user brushes the side-by-side bars, will they be able to select individual bars or will brushing one select all of them? 

## Combining parts

Finally, as displayed in Figure \@ref(fig:stacking), when multiple selections/groups are present, how do we combine them together? Again, in the case of the barplot of counts, there is an idiosyncratic way to do this: we can stack the counts across the selection groups and the corresponding bars on top of each other, and the height of the resulting bar will be identical to the count of the whole. And, as in the case of a single selection group, we can either draw the bars over each other, starting from the y-axis origin, or draw sub-bars stacked on top of each other, each starting where the last left off, and the resulting plots will be identical. 

In the case of barplot of means, there is no meaningful way to combine the selections. If we were to stack the bars on top of each other, the resulting statistic (i.e. the sum of the group means) would not be meaningful. Worse yet, if we were to take the mean of the group means, the resulting statistic will almost surely be different from the mean of the whole: the mean of the group means $\neq$ the grand mean. And again, the grand mean may be less than any one of the group means. Since we cannot meaningfully combine the statistics, we could draw the bars side-by-side, but again, we run into considerations about how to render the base group, the bar width, and the selection.

```{r stacking, echo=FALSE, fig.height=3, fig.cap=" Combining selections. a) An illustration of selection by linked brushing, with two groups. b) In the barplot of counts, statistics can be stacked on top of each other such that the count of the stacked bar is identical to the count of the whole bar (i.e. one that would result from no selections). c) In the barplot of means, no such procedure for combining statistics exists."}


wt <- mtcars$wt
mpg <- mtcars$mpg
cyl <- mtcars$cyl
disp <- mtcars$disp
one <- numeric(nrow(mtcars)) + 1

cyl_unique <- c(4, 6, 8)
group1 <- (wt > 2.5) & (wt < 4.5) & (mpg < 18.5)
group2 <- (wt > 2.5) & (wt < 4.5) & (mpg > 18.5)

group <- group1 * 1 + group2 * 2 + 1
cols <- c("grey80", "steelblue", "indianred")
col <- cols[group]


axis_fun1 <- function() {
  axis(1, tick = FALSE, line = -0.5)
  axis(2, tick = FALSE, las = 1, line = -0.5)
  box(bty = "L")
}

axis_fun2 <- function() {
  axis(1, at = c(4, 6, 8), tick = FALSE, line = -0.5)
  axis(2, tick = FALSE, las = 1, line = -0.5)
  box(bty = "L")
}

bars11 <- tapply(one, cyl, sum)
bars12 <- tapply(one, list(cyl, group), sum)
bars12[is.na(bars12)] <- 0

bars21 <- tapply(disp, cyl, mean)

par(mfrow = c(1, 3), mar = c(5, 4, 2, 0.5))

plot(wt, mpg, pch = 19, col = col, cex = 1.5,
     axes = FALSE, xlab = "Weight", ylab = "Mileage")
title(main = "a.", adj = 0, line = 0.5)
axis_fun1()

plot(c(2, 10), c(0, max(bars11)), type = "n",
     axes = FALSE, xlab = "Cylinders", ylab = "Count")
rect(cyl_unique - 0.75, 0, cyl_unique + 0.75, bars11, 
     col = "grey80", border = NA)
for (i in 1:3) {
  counts <- bars12[i, ]
  y0 <- cumsum(c(0, counts[-length(counts)]))
  y1 <- cumsum(counts)
  
  rect(cyl_unique[i] - 0.75, y0, cyl_unique[i] + 0.75, y1, 
       col = cols, border = NA)
} 
text(c(4, 4, 6, 6, 8, 8, 8), c(8, 11, 2, 7, 3, 12, 14) - 0.75, 
     labels = c("+8", "+3", "+2", "+5", "+3", "+9", "+2"), 
     col = c("grey40", "white", "white", "white", "grey40", 
             "white", "white"))
title(main = "b.", adj = 0, line = 0.5)
axis_fun2()

plot(c(2, 10), c(0, max(bars21)), type = "n",
     axes = FALSE, xlab = "Cylinders", ylab = "Mean displacement")
rect(cyl_unique - 0.75, 0, cyl_unique + 0.75, bars21, 
     col = "grey80", border = NA)
text(rep(c(4, 6, 8), each = 3), rep(25 * 1:3, 3), labels = "?",
     col = rep(c("grey40", "steelblue", "indianred"), 3))
title(main = "c.", adj = 0, line = 0.5)
axis_fun2()


```

## Some statistics are better than others

To summarize, counts, as implemented in a typical barplot, provide:

a. An unambiguous way to display empty selections (absence of a bar is a count of 0)
b. A stable visual reference (the count within a sub-bar $\leq$ the count within the whole bar)
c. A way to combine the statistics together (the sum of the sub-bar counts = the count of whole bar). 

Means do not share these nice properties: mean of an empty selection is not defined, the mean of a selection is not always subordinate to the mean of the whole, and the mean of group means is different from the grand mean. Importantly, these properties or lack thereof are not tied to any specific graphic (e.g. a barplot) but are instead tied to the underlying statistic (count/mean).

There are, of course, ways to display means with linked brushing/selection, and some have been described above. The key point, however, is that to display means, more decisions need to be made in order to produce a coherent interactive visualization. Consequently, since no choice is more "natural" than any other, a person who interacts with a linked  barplot of means for the first time may be surprised in how the plot behaves. For the barplot of counts, on the other hand, there *are* such natural solutions readily available, and this may explain why barplots of counts are a popular type of plot in systems which implement linked brushing. 

One might also wonder if these problems are just a quirk of linked brushing. However, first of all, the problems are not unique to interactive visualizations but arise in static plots as well, with stacking. Second, while they are perhaps the most prominent with linked brushing, we could run into them with any other interactive feature that deals with summary statistics on several parts of the data. If, for example, on top of linked brushing, we wanted to implement a pop-up window that displays summary statistics within a given object as text, we would still have to contend with the problem of what to do in the case of the mean of an empty selection. Here, there is perhaps a simple solution in displaying an `NA` or an empty string for the mean of empty selection. However, we should point out that by doing this, we have to go outside of the type of non-empty selections (real number/float). This may not be a problem in whatever statistical software we are using to render the plots. However, there is still something more natural about counts having the default value (0) be an element of the same type as all other values (integer).       

Returning to the nice properties of counts, are counts uniquely "good" in this regard? The short answer is "no". For example, sums or products (of values $\geq 1$) conform to these same nice properties of counts. In other words, in the context of linked brushing, a stacked barplot will behave equally well if it display sums (not surprising) or products (perhaps somewhat surprising?). In fact, there is whole a class of mathematical functions, or more precisely mathematical objects, that share these nice properties. To discuss these, let's first lay the groundwork with some relevant theory.   

# Few Relevant Bits of Category Theory

## Functions

A function is a mapping between two sets. More specifically, given the set of sources $S$ (also called the *domain*) and the set of possible targets $T$ (also called the *codomain*), we can think of a function as a subset $F \subseteq S \times T$ of valid source-target pairs $(s, t)$, such that for every $s \in S$ in there exists a unique $t \in T$ with $(s, t) \in F$. The function then can be thought of as "selecting" a $t$ for any valid $s$ it is given.

If a function $f$ covers all of its codomain, i.e. for all $t \in T$ there exists a $s \in S$ such that $f(s) = t$, then it is a *surjective* or *onto* function. If no two sources lead to the same target, i.e. for $s_1, s_2 \in S$, if $f(s_1) = t$ and $f(s_2) = t$, then $s_1 = s_2$, then it is an *injective* or *one-to-one* function. Also, for any given subset of targets, we can ask about the subset of sources or *pre-image* that could have produced them, i.e. for $T_i \subseteq T$ we can define $f^{-1}(T_i) = \{ s \in S \lvert f(s) \in T_i \}$. Finally, functions can be composed together: if we have two functions $f: X \to Y$ and $g: Y \to Z$, we can combine them into new function $h = g \circ f$ such that $h: X \to Z$, i.e. $h(x) = g(f(x))$.

## Partitions

One useful thing we can do with functions is to form partitions. Specifically, given some arbitrary set $A$, we can assign every element a label from a set of part labels $P$ via a surjective function $f : A \to P$. Conversely, we can then take any part label $p \in P$ and recover the corresponding subset of $A$ by pulling out its pre-image: $f^{-1}(p) = A_p \subseteq A$. We can use this to define partitions in another way, without reference to $f$: a partition of $A$ consists of a set of part labels $P$, such that, for all $p \in P$, there is a non-empty subset $A_p$ and:

$$A = \bigcup_{p \in P} A_p \qquad \text{and} \qquad \text{if } p \neq q, \text{ then } A_p \cap A_q = \varnothing$$
I.e. the parts $A_p$ jointly cover the entirety of $A$ and parts cannot share any elements.

We can rank partitions by their coarseness. That is, for any set $A$, the coarsest partition is one with only one part label $P = \{ 1 \}$, such that each element of $A$ gets assigned $1$ as label. Conversely, the finest partition is one where each element gets assigned its own unique part label, such that $\lvert A \lvert = \lvert P \lvert$. Finally, given two partitions, we can form a finer (or at least as fine) partition by taking their intersection, i.e. by taking the set of all unique pairs of labels that co-occur for any $a \in A$ as the new part labels. For example, if $A = \{ 1, 2, 3 \}$ and partition 1 assigns part labels $P_1 = \{x, y \}$ to $A$ such that $f_1(1) = x, \; f(2) = x, \; f(3) = y$, and partition 2 assigns part labels $P_2 = \{ \rho, \sigma \}$ such that $f_2(1) = \rho, \; f_2(2) = \sigma, \; f_2(3) = \sigma$, then the intersection partition will have part labels $P_3 = \{ (x, \rho), (x, \sigma), (y, \sigma) \}$ such that $f_3(1) = (x, \rho), \; f_3(2) = (x, \sigma), \; f_3(3) = (y, \sigma)$.   

## Preorders

A preorder is a set $X$ equipped with a binary relation $\leq$ that conforms to two simple properties:

1. $x \leq x$ for all $x \in X$ (reflexivity)
2. if $x \leq y$ and $y \leq z$, then $x \leq z$, for all $x, y, z \in X$ (transitivity)

Simply speaking, this means that between any two elements in $X$, there either is a relation and the elements relate (one element is somehow "less than or equal" to the other), or the two elements do not relate. 

An example of a preorder is the family tree, with the underlying set being the set of family members: $X = \{  \textbf{daughter}, \textbf{son}, \textbf{mother}, \textbf{father}, \textbf{grandmother}, ... \}$ and the binary relation being ancestry or familial relation. Thus, for example, $\textbf{daughter} \leq \textbf{father}$, since the daughter is related to the father, and $\textbf{father} \leq \textbf{father}$, since a person is related to themselves. However, there is no relation ($\leq$) between $\textbf{father}$ and $\textbf{mother}$ since they are not related. Finally, since $\textbf{daughter} \leq \textbf{father}$ and $\textbf{father} \leq \textbf{grandmother}$, then, by reflexivity, $\textbf{daughter} \leq \textbf{grandmother}$.

We can further restrict preorders by imposing additional properties, such as:

3. If $x \leq y$ and $y \leq x$, then $x = y$ (anti-symmetry)
4. Either $x \leq y$ or $y \leq x$ (comparability)

If a preorder conforms to 3., we speak of a partially ordered set or *poset*. If it conforms to both 3. and 4., then it is a *total order*. 

## Monoids

A monoid is a tuple $(M, e, \otimes)$ consisting of:

a. A set of objects $M$
b. A neutral element $e$ called the *monoidal unit*
c. A binary function $\otimes: M \times M \to M$ called the *monoidal product*

Such that:

1. $m \otimes e = e \otimes m = m$ for all $m \in M$ (unitality)
2. $m_1 \otimes (m_2 \otimes m_3) = (m_1 \otimes m_2) \otimes m_3 = m_1 \otimes m_2 \otimes m_3$ (associativity)

In simple terms, monoids encapsulate the idea that *the whole is exactly the "sum" of its parts* (where "sum" can be replaced by the monoidal product). Specifically, we have some elements and a way to combine them, and when we combine the same elements, no matter where we put the brackets we always get the same result (i.e. something like "the order does not matter", although that is not precisely right, more on that later). Finally, we have some neutral element that when combined with an element yields back the same element.  

For example, take summation on natural numbers, $(\mathbb{N}, 0, +)$:

$$1 + 0 = 0 + 1 = 1 \qquad \text{(unitality)}$$
$$1 + (2 + 3) = (1 + 2) + 3 = 1 + 2 + 3 \qquad \text{(associativity)}$$

Likewise, products of real numbers $(\mathbb{R}, 1, \times)$ are also a monoid, $(\mathbb{R}, 1, *)$, and so is multiplication of $n \times n$ square matrices $(\mathbf{M}_{n \in \mathbb{Z}}, \mathbf{I}, \cdot)$, where $\mathbf{I}$ is the identity matrix and $\cdot$ stands for an infix operator that is usually omitted. As a counterexample, exponentiation does not meet the definition of a monoid, since it is not associative: $x^{(y^z)} \neq (x^y)^z$. 

We can impose further restrictions on monoids, e.g.:

3. $m_1 \otimes m_2 = m_2 \otimes m_1$ for all $m \in M$ (commutativity)

Both commutativity and associativity can both be viewed as a kind of "order does not matter" rule, however, they are fundamentally different. Let's imagine our set of objects consists of three wires of different colours $\{ \textbf{red}, \textbf{green}, \textbf{blue} \}$ and the monoidal product consists of connecting wires. Let's also imagine that the $\textbf{red}$ wire is connected to a power source and the $\textbf{blue}$ wire is connected to a lightbulb, and the blue wire amplifies the current from the power source such that it is enough to power the light bulb. To turn on the lightbulb, we need to connect $\textbf{red} \to \textbf{green}$ and $\textbf{green} \to \textbf{blue}$. The time order in which we connect the three wires does not matter: we can connect $\textbf{green} \to \textbf{blue}$ first and $\textbf{red} \to \textbf{green}$ second or vice versa, either way we get the same result (lightbulb turns on). However, the spatial order in which we connect the wires *does* matter: if we connect $\textbf{red} \to \textbf{blue}$, then the current will not be enough to power the lightbulb. Hence, the operation is associative (temporal order does not matter) but not commutative (spatial order does matter).      


# The Model

Armed now with some rudimentary theory, we can attempt to describe the data visualization process in more detail. 

## Data and Partitions

We again start with the data $D$ which is just some set of values that may or may not be structured. Ultimately, our goal is to try and learn something new from the data. To this end, we want to create a visualization that will show the information from the data through graphical elements, primarily geometric objects such as points, lines, or areas (we may also use, for example, text/symbols). 

First of all, we need to think about how the information from the data will be distributed among the objects. To start with the simplest case, sometimes we might want all information in the data to go into a single geometric object. This may be a perfectly good way to display information about the entire data set. Examples of plots with a single geometric object may include time series plot (line), density plot (density polygon), or radar chart (pentagon/other polygon).

However, often, we want to compare and contrast different parts of the data. And this is where we have to start to think more deeply about how information should be allocated. For example, it would not be very useful to draw multiple identical geometric objects representing the same information from the data - if all of the objects looked the same, we could learn the same information from any one of them and so it is redundant to have more than one. Instead, we want each object to represent different aspects of the data and so we need to think about slicing the data up along some axis or axes.  

Often, the axes along which we can slice the data can already be present in its structure. For example, a lot of data comes organized in a tabular $R \times C$ format where $R$ is the set of rows and $C$ is the set of columns (and $R \times C$ are the cells), and so we could use any of these three sets as the basis for drawing our geometric objects. This is shown in Figure \@ref(fig:partitioning). Notice that, when slicing along rows $R$, the number of objects (points) is equal to their cardinality: $\textbf{\# objects} = \lvert R \lvert$. Likewise, when slicing along columns, $\textbf{\# objects} = \lvert C \lvert$, and when slicing along cells, $\textbf{\# objects} = \lvert R \times C \lvert$.

```{r partitioning, echo=FALSE, fig.height=4, fig.cap="Three different ways of partitioning a tabular data set. a) The underlying data set, represented as a matrix with 3 rows and 2 columns. b) In scatterplot, we can slice along the rows and draw points at the x- and y-position given by the columns. c) In lineplot, we can slice along the columns & draw lines by connecting points given by the corresponding row values. d) In a dotplot, we can slice along both rows and columns and draw points at given row and column index, with point size given by the corresponding cell value."}

library(gridExtra)

set.seed(123)
data <- matrix(sample(1:6), ncol = 2)

lims <- range(data)
scatlims <- apply(data, 2, range) + c(-1, 1)

lay1 <- matrix(c(1, 3, 2, 4), ncol = 2)
layout(lay1)
par(mar = c(2.5, 2.5, 1.5, 1.5))

plot(rep(1:2, 3), rep(1:3, 2), type = "n", axes = FALSE,
     xlab = NA, ylab = NA, xlim = c(0.5, 2.5), ylim = c(0.5, 3.5))
abline(v = 1.5, col = "grey60")
abline(h = c(1.5, 2.5), col = "grey60")
text(label = data[3:1, ], rep(1:2, 3), rep(1:3, 2))
box(col = "grey60")
title(main = "a.", adj = 0, line = 0.5)
title(ylab = "Rows", line = 0.5)
title(xlab = "Columns", line = 0.5)

plot(data[, 1], data[, 2], pch = 19, col = "steelblue",
     axes = FALSE, xlab = NA, ylab = NA,
     xlim = scatlims[, 1], ylim = scatlims[, 2])
axis(1, line = -3/4, tick = FALSE)
axis(2, line = -3/4, tick = FALSE, las = 1)
title(xlab = "Column 1", line = 1.5)
title(ylab = "Column 2", line = 1.5)
title(main = "b.", adj = 0, line = 0.5)
box(bty = "L", col = "grey60")

plot(data[, 1], type = "l", ylim = lims,
     xlab = NA, ylab = NA,
     axes = FALSE, col = "steelblue")
lines(data[, 2], ylim = lims, col = "steelblue")
axis(1, 1:3, line = -3/4, tick = FALSE)
axis(2, line = -3/4, tick = FALSE, las = 1)
title(xlab = "Row index", line = 1.5)
title(ylab = "Column", line = 1.5)
title(main = "c.", adj = 0, line = 0.5)
box(bty = "L", col = "grey60")

plot(rep(1:3, 2), rep(1:2, 3), 
     pch = 19, cex = data / sqrt(2), 
     xlim = c(0, 4), ylim = c(0, 3),
     col = "steelblue", axes = FALSE, 
     xlab = NA, ylab = NA)
axis(1, 1:3, line = -3/4, tick = FALSE)
axis(2, 1:2, line = -3/4, tick = FALSE, las = 1)
title(main = "d.", adj = 0, line = 0.5)
title(xlab = "Column index", line = 1.5)
title(ylab = "Row index", line = 1.5)
box(bty = "L", col = "grey60")

```

However, there is no reason to limit ourselves to slicing along axes that are already present in the structure of the data - we can also create new axes. For example, when computing the statistics underlying the classical barplot, we can slice along the levels of some discrete variable $x$. Likewise, when drawing a histogram or a heatmap, we can bin some continuous variable(s) and slice along the bins. If we are making an interactive visualization, we can make the slicing axis responsive to user input, for example, we might let the user change the histogram bin width and anchor interactively. 

Now, when slicing along any given axis or axes, a question remains *how* to slice. There are two key points to mention. Firstly, no matter how we perform the slicing, every unit of data $d \in D$ should probably end up in *some* slice, i.e. our slicing operation should not discard or "throw away" any data. This seems obvious - we want the visualization to be an accurate and complete representation of the data, and as such we cannot simply leave any unit of data out arbitrarily. Secondly, we also probably want each unit of the data to end up in *one slice only*. This point is a bit more subtle. Suppose, for example, that we were tasked with plotting data for a company, and we were given as data a set of $\textbf{employees}$ and a set of $\textbf{managers}$, with every manager in $\textbf{managers}$ also being included in the set of $\textbf{employees}$ (since they are also an employee of a company). Then, we could use the two sets as the two "slices" of that data and display the size/count of each set via the height of a bar in a barplot (such that the $\textbf{employees}$ bar would be at least as tall as the $\textbf{managers}$ bar, if all employees were managers). However, while it is certainly possible to create this type of plot, it unnecessarily duplicates information, and any questions we might want to answer with it might be better served by other plots. For example, if we wanted to compare the number of $\textbf{managers}$ vs. $\textbf{non-manager employees}$, we would be off drawing each as a separate bar. If we wanted to see what proportion of the total workforce are $\textbf{managers}$, we could draw one stacked bar with $\textbf{non-manager employees}$ stacked on top of $\textbf{managers}$.          

So, when slicing data, we want to make sure every unit of data ends up in one slice and no two units end up in the same slice. One might notice that this description of slicing corresponds precisely to the mathematical definition of a partition (Section \@ref(partitions)). That is, the data is split into parts and each unit of data is assigned to one part and one part only. This way of thinking about slicing the data is useful since we can make use of the dual nature of the definition of partition: we can either think of the partition as a function that assigns each data unit a part label $p \in P$, or as the set of part labels $p \in P$ where each has a corresponding subset of the data $D_p$, such that the union of the subsets recovers the original data set, $\bigcup_{p \in P} D_p = D$, and intersection of any two subsets is empty: $D_p \cap D_q = \varnothing , \; \text{for} p \neq q$. 

For illustration, when implementing a partition in a programming language, we could implement a partition either as an array of part labels where the array index corresponds to row index (e.g. $[A, A, B, A, B, C, \ldots]$) or as a dictionary of row indices (represented as either arrays or sets), indexed by part labels (e.g. $\{ A: [0, 1, 3, \ldots], B: [2, 4, \ldots], C: [5, \ldots] \}$). Either implementation may be more or less useful in different scenarios. For example, when computing some statistics for each part $p \in \{ A, B, C\}$, the array representation may be preferable since we have to loop through all rows of the data anyway (looping through an array is $O(n)$ operation, looping through all $k$ sub-arrays in the dictionary is also $O(n)$ but with some additional overhead for setting up the $k$ loops). If instead we wanted to select all indices belonging to part $A$, the dictionary representation will provide a more efficient $O(1)$ access, instead of having iterate over the entire array to pick out $A$ labels ($O(n)$). Thus, the data structure which we will use to implement the partitioning should reflect this duality, perhaps by internally implementing both the array and the dictionary representation.     

Finally, as with the mathematical definition, we can also form finer partitions from coarser ones by taking taking their intersection. Importantly, we can also think of taking the intersection of two partitions as hierarchically nesting one partition within the other. By starting with the coarsest partition, i.e. one that assigns each row of the data to the same part, and then progressively nesting factors within one another, we can build up a hierarchy of partitions. For example, if our data includes the variables $\textbf{gender} = \{ \textbf{male}, \textbf{male}, \textbf{female}, \textbf{male}, \textbf{female} \ldots \}$ and $\textbf{group} = \{ \textbf{A}, \textbf{B}, \textbf{A}, \textbf{B}, \textbf{B}, \ldots \}$, we might build up a hierarchy of partitions with labels $P_1$, $P_2$, and $P_3$ where $P_1$ is the set of part labels for the whole dataset $= \{ ( \; ) \}$, $P_2 = labels_{\textbf{gender}} = \{ (\textbf{male}), (\textbf{female}) \}$, and $P_3 = labels_{\textbf{gender} \times \textbf{group}} = \{ (\textbf{male}, \textbf{A}), (\textbf{male}, \textbf{B}), \ldots \}$.

## Computing Statistics

Now that the data has been split into parts, within possibly hierarchical/nested partitions, we need to think about computing some summary statistics on each part that we will then use to define the graphical attributes (length, area, angle,...) of the corresponding geometric objects. To do this, we will need to iterate across the data units $d \in D$ and reduce them into $k$ parts, where $k$ represents the cardinality of the partition at hand, $k = \lvert P \lvert$. Crucially, these statistics are not yet graphical attributes themselves, but will be turned into graphical coordinates either via a simple translation or by further computation. 





## Create Partitions: Partition

First, we need to split the data into partitions. We can do this directly by treating some variables in the data as sets of part labels or *factors*, and indeed this will be the typical use case for discrete variables. Alternatively, we can also turn continuous variables into factors by e.g. binning or truncation, as in the case of a histogram or a heatmap. Either way, a convenient way to represent these factors is as tuples $(indices, labels, metadata)$, where $indices$ is an integer vector of length $n$ (= the number of rows in the data) and values $j = 1, \ldots, k$, with $i$th index of value $j$ assigning the $i$th row of data to $j$th part, $labels$ is a dictionary assigning each $j$th part a collection of part labels consisting of a set of key-value pairs (e.g. "level", "binMin", "binMax"), and $metadata$ being additional information shared across parts, also represented as key-value pairs (e.g. the list of all bin breaks within a histogram).    




## Compute Statistics: Reduce

Second, for each $j$th part in a partition $J$, we need to compute a collection of summary statistics $s_j \in S$. If the summarizing functions that are used adhere to the monoidal contract (e.g. count, sum, product), this can be done in a single pass through the data or a *reduce* function, by looping through $n$ rows of the data and for each $i$th row updating the $j$th part, where $j$ is the corresponding $i$th factor index. If the summary statistic is not monoidal, we may be still able to compute auxiliary summary statistics that may be later used (in the *map* step) to compute the non-monoidal summary statistic, although breaking the monoidal contract in this way does come with the disadvantage of the representations no longer being guaranteed to be consistent.     

## Translate Statistics to Coordinates: Map

Third, we need to translate each collection of summary statistics $s_j \in S$ into a collection of graphical coordinates $g_j \in G$.  

## Combine Coordinates: Stack


# Something else

Symmetric monoidal preorders also come with some4 computational advantages. Firstly, these summaries are guaranteed to be computable within a single pass through the data, and can also be updated online if new data arrives, without having to refer to the old. This does not necessarily make them superior to other summaries, such as mean or variance, which can also often be computed by collecting multiple constituent summaries in a single pass through the data and them combining them together in one step after (such as sum and count for the mean), and for which online algorithms also often exist too. However, the advantage is that every symmetric monoidal preorder is automatically single pass computable and online - we will never need to look for an algorithm. Secondly, if our plot implements reactive axis limits, since the count within the selection is always guaranteed to be less that or equal to the count within the whole, we only need to keep track of the counts on the whole. For example, in the case of the barplot of counts, we know that the upper y-axis limit will always be equal to the height of the tallest whole bar, and we can safely ignore the sub-bars. If there are $N$ levels of the categorical variable and $K$ selection groups, there will be $N$ whole bars and $N \times K$ sub-bars. This difference might unimportant if $N$ and $K$ are small (as they are likely to be, in the case of the average barplot). However, if there are, for example, two categorical variables with $N$, and $M$ levels, respectively, as in the case of a fluctuation diagram/bubbleplot/treemap, and both $N$ and $M$ have many levels, then having to iterate through all $N \times M \times K$ sub-bar summaries every time an interaction happens might become prohibitive.   

```{r process, out.width="6in", out.height="3in", fig.align='center', echo=FALSE}

knitr::include_graphics("process.png")

```